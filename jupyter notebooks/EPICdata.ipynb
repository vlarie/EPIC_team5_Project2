{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmed needed dependencies\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Dependencies for geocoordinates generator\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import gmplot\n",
    "\n",
    "# Dependencies for conversion of coordinates to addresses\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "# Dependencies for Zillow data\n",
    "from pyzillow.pyzillow import ZillowWrapper, GetDeepSearchResults, GetUpdatedPropertyDetails\n",
    "\n",
    "# Dependency for Heat Mapper\n",
    "import gmaps\n",
    "\n",
    "\n",
    "# Add config.py file with the following variables and cooresponding Zillow API keys\n",
    "from config import Zapi, Zapi01, Zapi02, Zapi03, Zapi04, Zapi05, Zapi06, Zapi07, Zapi08, Zapi09, Zapi10, Zapi11, Zapi12, Zapi13, Zapi14, Zapi15, Zapi16, Zapi17, Zapi18, Zapi19, Zapi20, Ztroy1, Ztroy2, Ztroy3, Zseth, Zseth2, Zkat, Zval, Zyuta\n",
    "# from config import google_API_Key\n",
    "from config import census_API_Key\n",
    "\n",
    "################# ONGOING EDITS TO REQUIREMENTS.MD #################\n",
    "###### IF ANY ERRORS OCCUR RELATING TO MODULES OR CONFIG.PY #######\n",
    "### REFER TO requirements.md TO ENSURE YOU ARE PROPERLY SETUP ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File inputs/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These routes have changed and must be updated before using\n",
    "\n",
    "# masterDataCLEAN_csv = \"./Data/masterDataCLEAN.csv\"\n",
    "\n",
    "\n",
    "# randLatLon_csv = \"./Data/randomLatLon.csv\" \n",
    "# addressList_csv = \"./Data/addressList.csv\"\n",
    "# masterData_csv = \"./Data/masterData.csv\"\n",
    "# masterDFIMPORTclean_csv = \"./Data/masterDFIMPORTclean.csv\"\n",
    "# masterData100_csv = \"./Data/masterData100.csv\"\n",
    "# masterData1000_csv = \"./Data/masterData1000.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "##### VALERIE'S BLOCKS #####\n",
    "###########################\n",
    "\n",
    "# Funtion for reading CSV in as DataFrame\n",
    "def csvDF(oldCSVfilepath):\n",
    "    csvIN = pd.read_csv(oldCSVfilepath)\n",
    "    DF = pd.DataFrame(csvIN)\n",
    "    return DF\n",
    "\n",
    "# Function for converting DataFrame to CSV\n",
    "def DFcsv(dataframe, newCSVfilepath):\n",
    "    dataframe.to_csv(newCSVfilepath, index=False, header=True)\n",
    "    print(f\"Successfully written to '{newCSVfilepath}'\")\n",
    "    \n",
    "# Function for reading in csv, checking for headers, and appending if appropriate\n",
    "def csvDFappend(oldCSVfilepath, newColumn):\n",
    "    csvIN = pd.read_csv(oldCSVfilepath)\n",
    "    DF = pd.DataFrame(csvIN)\n",
    "    # Checking to ensure new header name does not match any current headers\n",
    "    colNames = DF.columns\n",
    "    for value in colNames:\n",
    "        if value == newColumn:\n",
    "            print(\"Cannot append column that matches an existing column name\")\n",
    "            return DF\n",
    "    # Check to ensure length of newColumn matches length of current dataframe columns\n",
    "    if len(newColumn) != len(DF):\n",
    "        print(\"Cannot append column that is not the same length as existing dataframe\")\n",
    "        return DF\n",
    "    # Append newColumn to Dataframe\n",
    "    DF[newColumn] = newColumn\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocoordinates of Austin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# this section written by troy bailey.   #\n",
    "# enter uservariables below to determine #\n",
    "# center location, radius of circle, and #\n",
    "# number of geocoordinates to generate.  #\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#### USER VARIABLES ####\n",
    "########################\n",
    "\n",
    "x0 = 30.27444       #### Set center coordiantes in decimal degrees\n",
    "y0 = -97.74028      #### initial coordiantes are location of Texas State Capitol Building\n",
    "\n",
    "radius = 20         #### Set radius in miles\n",
    "\n",
    "points = 40000        #### Set number of lat,lon points to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables and inputs for coordinate calculations\n",
    "lat_lon_list = []\n",
    "radiusInDegrees=radius/69           \n",
    "r = radiusInDegrees\n",
    "points += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate each coordiante point and build a list of lat and lon\n",
    "for i in range(1, points):\n",
    "    u = float(random.uniform(0.0,1.0)) #random number for radius length\n",
    "    v = float(random.uniform(0.0,1.0)) #random number for pi radians\n",
    "    \n",
    "    w = r * math.sqrt(u) #radius length\n",
    "    t = 2 * math.pi * v  #radians\n",
    "    x = w * math.cos(t)  #calculate x coord distance\n",
    "    y = w * math.sin(t)  #calculate y coord distance\n",
    "    \n",
    "    xLat  = x + x0       #offset x by center x\n",
    "    yLon = y + y0        #offset y by center y\n",
    "    \n",
    "    lat_lon_list.append([xLat,yLon])\n",
    "\n",
    "# convert list to dataframe\n",
    "lat_lon_df = pd.DataFrame(lat_lon_list, columns=['lat','lon'])\n",
    "\n",
    "lat_lon_df.head()\n",
    "\n",
    "len(lat_lon_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a CSV file of coordinate points\n",
    "lat_lon_df.to_csv(randLatLon_csv, index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot coordinate points on map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell by Troy\n",
    "# This section will plot points on a Google map centered at centerPointLat and centerPointLon with a magnification of magFactor\n",
    "# It assumes there is a dataframe with \"lat\" and \"lon\" columns\n",
    "# The resulting map is saved to a file called \n",
    "\n",
    "lat_lon_df = pd.read_csv(\"./Data/Archived/randomLatLon.csv\")\n",
    "centerPointLat = 30.27444  #these are the coordinates of the Texas State Capitol building\n",
    "centerPointLon = -97.74028 #these are the coordinates of the Texas State Capitol building\n",
    "magnificationFactor = 10\n",
    "pointColor = \"red\"\n",
    "pointSize = 100\n",
    "mapOutputFile = \"randLatLonMap.html\"\n",
    "df = lat_lon_df\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(centerPointLat, centerPointLon, magnificationFactor)\n",
    "\n",
    "gmap.scatter(df[\"lat\"], df[\"lon\"], pointColor, size=pointSize, marker=False)\n",
    "\n",
    "gmap.draw(\"./Presentation/\" + mapOutputFile)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Coordinates to Residential Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "##### Yuta's Blocks #####\n",
    "#########################\n",
    "\n",
    "##### Geopy Nominatim API #####\n",
    "geopy.geocoders.options.default_user_agent = \"ut-group-EPIC\"\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "url = \"https://nominatim.openstreetmap.org/reverse?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API - Known Residential Address\n",
    "params_1 = {\n",
    "    \"format\": \"jsonv2\",\n",
    "    \"lat\": 30.440777,\n",
    "    \"lon\": -97.777048\n",
    "}\n",
    "\n",
    "print(\"===== Test Home Response:\")\n",
    "response = requests.get(url, params=params_1).json()\n",
    "pp.pprint(response)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CSV, put into DataFrame\n",
    "latlon_df = pd.read_csv(randLatLon_csv)\n",
    "latlon_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put latitudes and longitudes into a zip object\n",
    "lats = latlon_df.iloc[:, 0]\n",
    "lons = latlon_df.iloc[:, 1]\n",
    "lat_lons = []\n",
    "lat_lons = zip(lats, lons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Loop Request API for Addresses / Append to lists #####\n",
    "# Make sure to import time\n",
    "\n",
    "query_url = \"https://nominatim.openstreetmap.org/reverse?\"\n",
    "\n",
    "house_num = []\n",
    "road = []\n",
    "postcode = []\n",
    "lat = []\n",
    "lon = []\n",
    "neighborhood = []\n",
    "\n",
    "counter = 1\n",
    "numRequests = latlon_df[\"lat\"].count()\n",
    "rSuccess = []\n",
    "rFailure = []\n",
    "\n",
    "print(f\"Processing {numRequests} Requests...\")\n",
    "\n",
    "# Nominatim API Request\n",
    "\n",
    "for lat_lon in lat_lons:\n",
    "    params = {\n",
    "        \"format\": \"jsonv2\",\n",
    "        \"lat\": lat_lon[0],\n",
    "        \"lon\": lat_lon[1]\n",
    "    }\n",
    "\n",
    "    time.sleep(1.1)\n",
    "    response = requests.get(query_url, params=params).json()\n",
    "\n",
    "    if response['type'] == 'house' or response['type'] == 'yes':\n",
    "        lat.append(response['lat'])\n",
    "        lon.append(response['lon'])\n",
    "        \n",
    "        try:\n",
    "            postcode.append(response['address']['postcode'])\n",
    "        except (KeyError, IndexError):\n",
    "            postcode.append(\"NA\")\n",
    "        try:\n",
    "            house_num.append(response['address']['house_number'])\n",
    "        except (KeyError, IndexError):\n",
    "            house_num.append(\"NA\")\n",
    "        try:\n",
    "            road.append(response['address']['road'])\n",
    "        except (KeyError, IndexError):\n",
    "            road.append(\"NA\")\n",
    "        try:\n",
    "            neighborhood.append(response['address']['neighbourhood'])\n",
    "        except (KeyError, IndexError):\n",
    "            neighborhood.append(\"NA\")\n",
    "        \n",
    "        print(f\"Processed Record {counter} of {numRequests}.\")\n",
    "        rSuccess.append(counter)\n",
    "        counter += 1\n",
    "        \n",
    "    else:\n",
    "        print(f\"Wrong Type - Skipped Record {counter} of {numRequests}.\")\n",
    "        rFailure.append(counter)\n",
    "        counter += 1\n",
    "        \n",
    "print(f\"Finished Requests !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Request Results:\")\n",
    "print(\"Success #:\" + str(len(rSuccess)))\n",
    "print(\"Skipped #:\" + str(len(rFailure)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with addresses from API requests\n",
    "address_df = pd.DataFrame({\n",
    "    \"house #\": house_num,\n",
    "    \"street\": road,\n",
    "    \"zipcode\": postcode,\n",
    "    \"lat\": lat,\n",
    "    \"lon\": lon,\n",
    "    \"neighborhood\": neighborhood,\n",
    "})\n",
    "\n",
    "# Clean up Dataframe Columns before output (Drop incomplete zipcodes, Highway streets, and Null house # or streets)\n",
    "address_df = address_df[address_df['zipcode'].str.len() == 5]\n",
    "address_df = address_df[address_df['zipcode'].apply(lambda x: len(str(x)) > 3)]\n",
    "address_df = address_df[address_df['street'].str.contains(\"Highway\") == False]\n",
    "address_df = address_df[address_df['house #'].str.contains(\"NA\") == False]\n",
    "address_df = address_df[address_df['street'].str.contains(\"NA\") == False]\n",
    "address_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a CSV file of addresses\n",
    "address_df.to_csv(addressList_csv, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map out CSV with gmplot\n",
    "\n",
    "addressList_csv = \"./Data/addressList.csv\"\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(30.27444, -97.74028, 10)\n",
    "\n",
    "gmap.scatter(addressList_csv[\"lat\"], addressList_csv[\"lon\"], 'red', size=20, marker=False)\n",
    "\n",
    "gmap.draw(\"./Visuals/myaddressmap.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Addresses on a Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell by Troy\n",
    "# This section will plot points on a Google map centered at centerPointLat and centerPointLon with a magnification of magFactor\n",
    "# It plots the addresses we have selected from the random Lat Lon points\n",
    "# The resulting map is saved to a file called addressMap.html\n",
    "\n",
    "address_df = pd.read_csv(\"./Data/Archived/addressList.csv\")\n",
    "centerPointLat = 30.27444  #these are the coordinates of the Texas State Capitol building\n",
    "centerPointLon = -97.74028 #these are the coordinates of the Texas State Capitol building\n",
    "magnificationFactor = 10\n",
    "pointColor = \"blue\"\n",
    "pointSize = 100\n",
    "mapOutputFile = \"addressMap.html\"\n",
    "df = address_df\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(centerPointLat, centerPointLon, magnificationFactor)\n",
    "\n",
    "gmap.scatter(df[\"lat\"], df[\"lon\"], pointColor, size=pointSize, marker=False)\n",
    "\n",
    "gmap.draw(\"./Presentation/\" + mapOutputFile)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zillow API Calls using Address and Zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "#############################\n",
    "##### VALERIE'S BLOCKS #####\n",
    "###########################\n",
    "\n",
    "# Funtion for reading CSV in as DataFrame\n",
    "def csvDF(oldCSVfilepath):\n",
    "    csvIN = pd.read_csv(oldCSVfilepath)\n",
    "    DF = pd.DataFrame(csvIN)\n",
    "    return DF\n",
    "\n",
    "# Function for converting DataFrame to CSV\n",
    "def DFcsv(dataframe, newCSVfilepath):\n",
    "    dataframe.to_csv(newCSVfilepath, index=False, header=True)\n",
    "    print(f\"Successfully written to '{newCSVfilepath}'\")\n",
    "    \n",
    "# Function for reading in csv, checking for headers, and appending if appropriate\n",
    "def csvDFappend(oldCSVfilepath, newColumn):\n",
    "    csvIN = pd.read_csv(oldCSVfilepath)\n",
    "    DF = pd.DataFrame(csvIN)\n",
    "    # Checking to ensure new header name does not match any current headers\n",
    "    colNames = DF.columns\n",
    "    for value in colNames:\n",
    "        if value == newColumn:\n",
    "            print(\"Cannot append column that matches an existing column name\")\n",
    "            return DF\n",
    "    # Check to ensure length of newColumn matches length of current dataframe columns\n",
    "    if len(newColumn) != len(DF):\n",
    "        print(\"Cannot append column that is not the same length as existing dataframe\")\n",
    "        return DF\n",
    "    # Append newColumn to Dataframe\n",
    "    DF[newColumn] = newColumn\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# Read in address list to run through Zillow API \n",
    "addressDF = csvDF(\"../data/Archived/addressList.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "print(len(addressDF))\n",
    "addressDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# Drop duplicates prior to running Zillow API call\n",
    "addressDF = addressDF.drop_duplicates()\n",
    "print(len(addressDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "############### LOOPING FUNCTION FULLY OPERATIONAL ################\n",
    "####### HOWEVER, ZILLOW ONLY ALLOWS 1000 API CALLS PER DAY #######\n",
    "\n",
    "# Zillow API call function using address and zipcode\n",
    "def zCall(API, index, address, zipcode):\n",
    "    APIkey = API[index]\n",
    "    zillow_data = ZillowWrapper(APIkey)\n",
    "    deep_search_response = zillow_data.get_deep_search_results(address, zipcode)\n",
    "    result = GetDeepSearchResults(deep_search_response)\n",
    "    return result\n",
    "\n",
    "\n",
    "# List containers for collected property data\n",
    "zid = []\n",
    "addresses = []\n",
    "alats = []\n",
    "alons = []\n",
    "valuation = []\n",
    "valChange = []\n",
    "valRhigh = []\n",
    "valRlow = []\n",
    "sqft = []\n",
    "lotsqft = []\n",
    "yearBuilt = []\n",
    "lastSold = []\n",
    "lastPrice = []\n",
    "\n",
    "\n",
    "# List of Zillow API keys to loop through due to daily API call limits\n",
    "zAPIs = [Zapi, Zapi01, Zapi02, Zapi03, Zapi04, Zapi05, Zapi06, Zapi07, Zapi08, Zapi09, \n",
    "         Zapi10, Zapi11, Zapi12, Zapi13, Zapi14, Zapi15, Zapi16, Zapi17, Zapi18, Zapi19, \n",
    "         Zapi20, Ztroy1, Ztroy2, Ztroy3, Zseth, Zseth2, Zkat, Zval, Zyuta]\n",
    "index = 0\n",
    "    \n",
    "for row, home in addressDF.iterrows():\n",
    "    address = str(addressDF[\"house #\"][row]) + \" \" + str(addressDF[\"street\"][row])\n",
    "    addresses.append(address)\n",
    "    zipcode = addressDF[\"zipcode\"][row]\n",
    "    print(f\"Processing {address}, {zipcode} (index {row}).\")\n",
    "    \n",
    "    result = None\n",
    "    try:\n",
    "        try:\n",
    "            result = zCall(zAPIs, index, address, zipcode)\n",
    "            print(f\"{row} Success!\")\n",
    "        except KeyError:  ### ERROR FOR API CALL LIMIT EXCEEDED ###\n",
    "            print(f\"KeyError has occurred for {address}, {zipcode} (index {row}).\")\n",
    "            index += 1\n",
    "            print(f\"Proceeding to API[{index}]\")\n",
    "            if index >= len(zAPIs):\n",
    "                print(f\"API[{index}] does not exist. Need more API keys to complete analysis.\")\n",
    "                break\n",
    "            result = zCall(zAPIs, index, address, zipcode)\n",
    "\n",
    "    except:\n",
    "        print(f\"No record found for {address}, {zipcode} (index {row}). Appending lists with null values\")\n",
    "        zid.append(None)\n",
    "        alats.append(None)\n",
    "        alons.append(None)\n",
    "        valuation.append(None)\n",
    "        valChange.append(None)\n",
    "        valRhigh.append(None)\n",
    "        valRlow.append(None)\n",
    "        sqft.append(None)\n",
    "        lotsqft.append(None)\n",
    "        yearBuilt.append(None)\n",
    "        lastSold.append(None)\n",
    "        lastPrice.append(None)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        zillowID = result.zillow_id\n",
    "        zid.append(zillowID)\n",
    "    except:\n",
    "        print(f\"No zid found for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        zid.append(None)\n",
    "\n",
    "    try:\n",
    "        alat = result.latitude\n",
    "        alats.append(alat)\n",
    "    except:\n",
    "        print(f\"No alat found for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        alats.append(None)\n",
    "\n",
    "    try:\n",
    "        alon = result.longitude\n",
    "        alons.append(alon)\n",
    "    except:\n",
    "        print(f\"No alon found for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        alons.append(None)\n",
    "\n",
    "    try:    \n",
    "        val = int(result.zestimate_amount)\n",
    "        valuation.append(val)\n",
    "    except:\n",
    "        print(f\"No valuation found for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        valuation.append(None)\n",
    "        \n",
    "    try:    \n",
    "        change = result.zestimate_value_change\n",
    "        valChange.append(change)\n",
    "    except:\n",
    "        print(f\"No valuation change found for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        valChange.append(None)\n",
    "        \n",
    "    try:    \n",
    "        high = result.zestimate_valuation_range_high\n",
    "        valRhigh.append(high)\n",
    "    except:\n",
    "        print(f\"No valuation range high found for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        valRhigh.append(None)\n",
    "\n",
    "    try:    \n",
    "        low = result.zestimate_valuationRange_low\n",
    "        valRlow.append(low)\n",
    "    except:\n",
    "        print(f\"No valuation range low found for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        valRlow.append(None)\n",
    "        \n",
    "    try:\n",
    "        zsqft = result.home_size\n",
    "        sqft.append(zsqft)\n",
    "    except:\n",
    "        print(f\"No sqft found for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        sqft.append(None)\n",
    "        \n",
    "    try:\n",
    "        lot = result.property_size\n",
    "        lotsqft.append(lot)\n",
    "    except:\n",
    "        print(f\"No lot size for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        lotsqft.append(None)\n",
    "        \n",
    "    try:\n",
    "        year = int(result.year_built)\n",
    "        yearBuilt.append(year)\n",
    "    except:\n",
    "        print(f\"No year built for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        yearBuilt.append(None)\n",
    "        \n",
    "    try:\n",
    "        last = result.last_sold_date\n",
    "        lastSold.append(last)\n",
    "    except:\n",
    "        print(f\"No last sold date for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        lastSold.append(None)\n",
    "        \n",
    "    try:\n",
    "        price = result.last_sold_price\n",
    "        lastPrice.append(price)\n",
    "    except:\n",
    "        print(f\"No last sold price for {address}, {zipcode} (index {row}). Appending list with null values\")\n",
    "        lastPrice.append(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Master Dataframe Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# Checking to ensure lengths of lists are identical\n",
    "print(len(zid))\n",
    "print(len(addresses))\n",
    "print(len(alats))\n",
    "print(len(alons))\n",
    "print(len(valuation))\n",
    "print(len(valChange))\n",
    "print(len(valRhigh))\n",
    "print(len(valRlow))\n",
    "print(len(sqft))\n",
    "print(len(lotsqft))\n",
    "print(len(yearBuilt))\n",
    "print(len(lastSold))\n",
    "print(len(lastPrice))\n",
    "\n",
    "# Referring back to addressList_csv generated dataframe for relevant info\n",
    "addressDF.head()\n",
    "print(len(addressDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "masterDF = pd.DataFrame({\n",
    "    \"zid\": zid,\n",
    "    \"address\": addresses,\n",
    "    \"zipcode\": addressDF[\"zipcode\"],\n",
    "    \"alat\": alats,\n",
    "    \"alon\": alons,\n",
    "    \"valuation\": valuation,\n",
    "    \"value_change\": valChange,\n",
    "    \"value_range_high\": valRhigh,\n",
    "    \"value_range_low\": valRlow,\n",
    "    \"sqft\": sqft,\n",
    "#     \"value_sqft\": valsqft,\n",
    "    \"lot_sqft\": lotsqft,\n",
    "    \"year_built\": yearBuilt,\n",
    "    \"last_sold_date\": lastSold,\n",
    "    \"last_sold_price\": lastPrice,\n",
    "})\n",
    "\n",
    "print(len(masterDF))\n",
    "masterDF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# masterDFclean to csv\n",
    "masterDF.to_csv(\"../data/zillowRaw.csv\", index=False, header=True)\n",
    "masterDF.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zillow Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "zillowDF = csvDF(\"../data/zillowRaw.csv\")\n",
    "print(len(zillowDF))\n",
    "zillowDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# DATA CLEANING #\n",
    "# Dropping duplicates\n",
    "masterDFdrops = zillowDF.drop_duplicates(subset=[\"zid\"], keep=\"first\")\n",
    "print(len(masterDFdrops))\n",
    "masterDFdrops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# Cleaning out None values for \"valuation\"\n",
    "masterDFdrops = masterDFdrops.dropna(how=\"any\", subset=[\"valuation\"])\n",
    "print(len(masterDFdrops))\n",
    "masterDFdrops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# Cleaning out None values for \"sqft\" \n",
    "masterDFdrops = masterDFdrops.dropna(how=\"any\", subset=[\"sqft\"])\n",
    "print(len(masterDFdrops))\n",
    "masterDFdrops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Value per Sqft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# Calculate \"value sqft\" after None value rows removed for \"valuation\" and \"sqft\"\n",
    "\n",
    "valsqft = []\n",
    "for row, value in masterDFdrops.iterrows():\n",
    "    try:\n",
    "        vsqft = round((masterDFdrops[\"valuation\"][row] / masterDFdrops[\"sqft\"][row]), 2)\n",
    "        valsqft.append(vsqft)\n",
    "    except: ### THIS ERROR SHOULD NO LONGER PRINT BECAUSE NONE VALUES WERE PREVIOUSLY REMOVED\n",
    "        print(\"Cannot perform math with NoneType\")\n",
    "        valsqft.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# Checking to ensure lists are appropriate lengths\n",
    "print(len(masterDFdrops))\n",
    "print(len(valsqft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# Adding \"value sqft\" column\n",
    "masterDFdrops[\"value_sqft\"] = valsqft\n",
    "\n",
    "# Reordering columns\n",
    "masterDFdrops = masterDFdrops[['zid', 'address', 'zipcode', 'alat', 'alon', 'valuation', 'value_change', 'value_range_high',\n",
    "                               'value_range_low', 'sqft', 'value_sqft', 'lot_sqft', 'year_built', 'last_sold_date', \n",
    "                               'last_sold_price']]\n",
    "#                                , 'tractCode', 'countyFips', 'stateFips', 'commuteTime']]\n",
    "masterDFdrops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# Checking high and low values with sort\n",
    "masterDFdrops.sort_values(by=\"value_sqft\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# Function for finding and dropping rows with nonsense values (e.g. valuation > $10,000,000)\n",
    "def dropNonsense(dataframe, columnName, minVal, maxVal):\n",
    "    dropIndices = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        val = dataframe[columnName][index]\n",
    "        if (maxVal is not None) and (val > maxVal) or (minVal is not None) and (val < minVal):\n",
    "            dropIndices.append(index)\n",
    "    return dataframe.drop(index=dropIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# Establish reasonable values for columns\n",
    "reasonableVals = [\n",
    "    (\"valuation\", 20000, 10000000),\n",
    "    (\"sqft\", 500, 10000),\n",
    "    (\"value_sqft\", 0, 1500)\n",
    "]\n",
    "\n",
    "# Loop through dataframe to drop nonsense data\n",
    "masterDFdrops = masterDFdrops\n",
    "for entry in reasonableVals:\n",
    "    masterDFdrops = dropNonsense(masterDFdrops, entry[0], entry[1], entry[2])\n",
    "\n",
    "# Confirm count\n",
    "print(len(masterDFdrops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDFdrops.to_csv(\"../data/zillowClean.csv\", index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# Save to file\n",
    "DFcsv(masterDFdrops, \"..data/zillowClean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Master Data with Commute Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commuteDF = csvDF(\"./Data/masterData.csv\")\n",
    "commuteDF.head()\n",
    "print(len(commuteDF))\n",
    "print(len(masterDFcleaning))\n",
    "print(\"Commute\")\n",
    "print(commuteDF.dtypes)\n",
    "print(\"Master\")\n",
    "print(masterDFcleaning.dtypes)\n",
    "commuteDF.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTERdf = pd.merge(masterDFcleaning, \n",
    "                    commuteDF[['tractCode', 'countyFips', 'stateFips', 'commuteTime']], \n",
    "                    how=\"left\",\n",
    "                    on='Zillow ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(MASTERdf))\n",
    "MASTERdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Zillow Matched Addresses on a Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell by Troy\n",
    "# This section will plot points on a Google map centered at centerPointLat and centerPointLon with a magnification of magFactor\n",
    "# It plots the Zillow addresses we have matched from our randomly selected \n",
    "# The resulting map is saved to a file called addressMap.html\n",
    "\n",
    "masterData_df = pd.read_csv(\"./Data/masterDataCLEAN.csv\")\n",
    "centerPointLat = 30.27444  #these are the coordinates of the Texas State Capitol building\n",
    "centerPointLon = -97.74028 #these are the coordinates of the Texas State Capitol building\n",
    "magnificationFactor = 10\n",
    "pointColor = \"green\"\n",
    "pointSize = 100\n",
    "mapOutputFile = \"masterDataMap.html\"\n",
    "df = masterData_df\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(centerPointLat, centerPointLon, magnificationFactor)\n",
    "\n",
    "gmap.scatter(df[\"alat\"], df[\"alon\"], pointColor, size=pointSize, marker=False)\n",
    "\n",
    "gmap.draw(\"./Presentation/\" + mapOutputFile)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeDF = csvDF(\"../data/Archived/annualCrimeData2016.csv\")\n",
    "crimeDF.columns\n",
    "print(len(crimeDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 78735.0\n",
    "print(a)\n",
    "a = int(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeDFdrops[\"zipcode\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crimeDFdrops[\"zipcode\"].nunique()\n",
    "counts = crimeDFdrops.dropna(how=\"any\", subset=[\"zipcode\"])\n",
    "print(len(crimeDFdrops))\n",
    "len(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeDFzip = counts.astype({\"zipcode\": int})\n",
    "crimeDFzip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeDFdrops = crimeDF.dropna(how=\"any\", subset=[\"GO X Coordinate\"])\n",
    "print(len(crimeDFdrops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeDFdrops = crimeDFdrops.dropna(how=\"any\", subset=[\"GO Y Coordinate\"])\n",
    "print(len(crimeDFdrops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert latitude and longitude to valid values\n",
    "lats = crimeDFdrops['GO X Coordinate']\n",
    "clats = []\n",
    "\n",
    "for coord in lats:\n",
    "    coord /= 100000\n",
    "    clats.append(coord)\n",
    "    \n",
    "# Confirm length of list    \n",
    "print(len(clats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert latitude and longitude to valid values\n",
    "lons = crimeDFdrops['GO Y Coordinate']\n",
    "clons = []\n",
    "\n",
    "for coord in lons:\n",
    "    coord /= 100000\n",
    "    clons.append(coord)\n",
    "    \n",
    "# Confirm length of list    \n",
    "print(len(clons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop previous coordinate columns\n",
    "crimeDFdrops = crimeDFdrops.drop(['Latitude', 'Longitude'], axis=1)\n",
    "\n",
    "print(crimeDFdrops.columns)\n",
    "crimeDFdrops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append adjusted coordinate columns\n",
    "crimeDFdrops[\"clat\"] = clats\n",
    "crimeDFdrops[\"clon\"] = clons\n",
    "crimeDFdrops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeDFdrops = crimeDFdrops.rename(columns = {'Highest NIBRS/UCR Offense Description': \"offenseCategory\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeDFdrops = crimeDFdrops.rename(columns = {'GO Highest Offense Desc': \"offenseDetails\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeDFdrops = crimeDFdrops.rename(columns = {'GO Location Zip': \"zipcode\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeDFdrops['offenseCategory'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeDFsorted = crimeDFdrops.sort_values('offenseCategory')\n",
    "crimeDFsorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeCounts = crimeDFsorted[\"offenseCategory\"].value_counts()\n",
    "print(len(crimeDFsorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeDFzip.to_csv(\"../data/crimeData.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in new crime file\n",
    "crimeDF = csvDF(\"../data/crimeData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GO Primary Key</th>\n",
       "      <th>Council District</th>\n",
       "      <th>offenseDetails</th>\n",
       "      <th>offenseCategory</th>\n",
       "      <th>GO Report Date</th>\n",
       "      <th>GO Location</th>\n",
       "      <th>Clearance Status</th>\n",
       "      <th>Clearance Date</th>\n",
       "      <th>GO District</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>GO Census Tract</th>\n",
       "      <th>clat</th>\n",
       "      <th>clon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201610188.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>AGG ASLT ENHANC STRANGL/SUFFOC</td>\n",
       "      <td>Agg Assault</td>\n",
       "      <td>01-Jan-16</td>\n",
       "      <td>8600 W SH 71</td>\n",
       "      <td>C</td>\n",
       "      <td>12-Jan-16</td>\n",
       "      <td>D</td>\n",
       "      <td>78735</td>\n",
       "      <td>19.08</td>\n",
       "      <td>30.67322</td>\n",
       "      <td>100.62796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201610643.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>THEFT</td>\n",
       "      <td>Theft</td>\n",
       "      <td>01-Jan-16</td>\n",
       "      <td>219 E 6TH ST</td>\n",
       "      <td>C</td>\n",
       "      <td>04-Jan-16</td>\n",
       "      <td>G</td>\n",
       "      <td>78701</td>\n",
       "      <td>11.00</td>\n",
       "      <td>31.14957</td>\n",
       "      <td>100.70462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201610892.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>AGG ROBBERY/DEADLY WEAPON</td>\n",
       "      <td>Robbery</td>\n",
       "      <td>01-Jan-16</td>\n",
       "      <td>701 W LONGSPUR BLVD</td>\n",
       "      <td>N</td>\n",
       "      <td>03-May-16</td>\n",
       "      <td>E</td>\n",
       "      <td>78753</td>\n",
       "      <td>18.23</td>\n",
       "      <td>31.29181</td>\n",
       "      <td>101.06923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201610893.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>THEFT</td>\n",
       "      <td>Theft</td>\n",
       "      <td>01-Jan-16</td>\n",
       "      <td>404 COLORADO ST</td>\n",
       "      <td>N</td>\n",
       "      <td>22-Jan-16</td>\n",
       "      <td>G</td>\n",
       "      <td>78701</td>\n",
       "      <td>11.00</td>\n",
       "      <td>31.13643</td>\n",
       "      <td>100.70357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201611148.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>DEADLY CONDUCT</td>\n",
       "      <td>Agg Assault</td>\n",
       "      <td>01-Jan-16</td>\n",
       "      <td>8002 TAPO LN</td>\n",
       "      <td>N</td>\n",
       "      <td>08-Jan-16</td>\n",
       "      <td>C</td>\n",
       "      <td>78724</td>\n",
       "      <td>22.08</td>\n",
       "      <td>31.46947</td>\n",
       "      <td>100.77985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GO Primary Key  Council District                  offenseDetails  \\\n",
       "0     201610188.0               8.0  AGG ASLT ENHANC STRANGL/SUFFOC   \n",
       "1     201610643.0               9.0  THEFT                            \n",
       "2     201610892.0               4.0  AGG ROBBERY/DEADLY WEAPON        \n",
       "3     201610893.0               9.0  THEFT                            \n",
       "4     201611148.0               1.0  DEADLY CONDUCT                   \n",
       "\n",
       "  offenseCategory GO Report Date          GO Location Clearance Status  \\\n",
       "0     Agg Assault      01-Jan-16         8600 W SH 71                C   \n",
       "1           Theft      01-Jan-16         219 E 6TH ST                C   \n",
       "2         Robbery      01-Jan-16  701 W LONGSPUR BLVD                N   \n",
       "3           Theft      01-Jan-16      404 COLORADO ST                N   \n",
       "4     Agg Assault      01-Jan-16         8002 TAPO LN                N   \n",
       "\n",
       "  Clearance Date GO District  zipcode  GO Census Tract      clat       clon  \n",
       "0      12-Jan-16           D    78735            19.08  30.67322  100.62796  \n",
       "1      04-Jan-16           G    78701            11.00  31.14957  100.70462  \n",
       "2      03-May-16           E    78753            18.23  31.29181  101.06923  \n",
       "3      22-Jan-16           G    78701            11.00  31.13643  100.70357  \n",
       "4      08-Jan-16           C    78724            22.08  31.46947  100.77985  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crimeDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in crime severity csv\n",
    "crimeSeverityDF = csvDF(\"../data/crimeDescSeverity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in crime severity csv\n",
    "crimeSeverityDF2 = csvDF(\"../data/crimeDataSeverity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "37444\n"
     ]
    }
   ],
   "source": [
    "print(len(crimeSeverityDF))\n",
    "print(len(crimeSeverityDF2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Offense Desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H+</td>\n",
       "      <td>4</td>\n",
       "      <td>AGG ASLT ENHANC STRANGL/SUFFOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H+</td>\n",
       "      <td>4</td>\n",
       "      <td>AGG ASLT STRANGLE/SUFFOCATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H+</td>\n",
       "      <td>4</td>\n",
       "      <td>AGG ASLT W/MOTOR VEH FAM/DAT V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H+</td>\n",
       "      <td>4</td>\n",
       "      <td>AGG ASSAULT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H+</td>\n",
       "      <td>4</td>\n",
       "      <td>AGG ASSAULT FAM/DATE VIOLENCE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Level  Severity                    Offense Desc\n",
       "0    H+         4  AGG ASLT ENHANC STRANGL/SUFFOC\n",
       "1    H+         4  AGG ASLT STRANGLE/SUFFOCATE   \n",
       "2    H+         4  AGG ASLT W/MOTOR VEH FAM/DAT V\n",
       "3    H+         4  AGG ASSAULT                   \n",
       "4    H+         4  AGG ASSAULT FAM/DATE VIOLENCE "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crimeSeverityDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>Severity</th>\n",
       "      <th>offenseDetails</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H+</td>\n",
       "      <td>4</td>\n",
       "      <td>AGG ASLT ENHANC STRANGL/SUFFOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H+</td>\n",
       "      <td>4</td>\n",
       "      <td>AGG ASLT STRANGLE/SUFFOCATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H+</td>\n",
       "      <td>4</td>\n",
       "      <td>AGG ASLT W/MOTOR VEH FAM/DAT V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H+</td>\n",
       "      <td>4</td>\n",
       "      <td>AGG ASSAULT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H+</td>\n",
       "      <td>4</td>\n",
       "      <td>AGG ASSAULT FAM/DATE VIOLENCE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Level  Severity                  offenseDetails\n",
       "0    H+         4  AGG ASLT ENHANC STRANGL/SUFFOC\n",
       "1    H+         4  AGG ASLT STRANGLE/SUFFOCATE   \n",
       "2    H+         4  AGG ASLT W/MOTOR VEH FAM/DAT V\n",
       "3    H+         4  AGG ASSAULT                   \n",
       "4    H+         4  AGG ASSAULT FAM/DATE VIOLENCE "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crimeSeverityDF = crimeSeverityDF.rename(columns = {'Offense Desc': \"offenseDetails\"})\n",
    "crimeSeverityDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can not merge DataFrame with instance of type <class 'pandas.core.series.Series'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2f3d1b875e8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m                        \u001b[0mcrimeSeverityDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Severity\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                        \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                        on=\"offenseDetails\")\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# MASTERdf = pd.merge(masterDFcleaning,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     58\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                          validate=validate)\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m             raise ValueError('can not merge DataFrame with instance of '\n\u001b[1;32m--> 526\u001b[1;33m                              'type {right}'.format(right=type(right)))\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_bool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: can not merge DataFrame with instance of type <class 'pandas.core.series.Series'>"
     ]
    }
   ],
   "source": [
    "fullCrimeDF = pd.merge(crimeDF, \n",
    "                       crimeSeverityDF[\"Severity\"], \n",
    "                       how=\"left\", \n",
    "                       on=\"offenseDetails\")\n",
    "\n",
    "# MASTERdf = pd.merge(masterDFcleaning, \n",
    "#                     commuteDF[['tractCode', 'countyFips', 'stateFips', 'commuteTime']], \n",
    "#                     how=\"left\",\n",
    "#                     on='Zillow ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36579\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "print(len(crimeDF))\n",
    "print(len(crimeSeverityDF))\n",
    "# print(len(fullCrimeDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## School Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seth's section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Commute Time Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# Troy's section\n",
    "\n",
    "# This section reads the masterData csv file and uses the lat lon coordinates to get the us census tract code for that address\n",
    "# It then uses the tract code to access the average commute times reported for that tract\n",
    "\n",
    "\n",
    "# Load in data frame from file with lat and lon\n",
    "masterData_df = csvDF(\"../data/zillowClean.csv\")\n",
    "\n",
    "\n",
    "lats = masterData_df['alat']\n",
    "lons = masterData_df['alon']\n",
    "\n",
    "# Set up arrays for new data\n",
    "tractCodeList = []\n",
    "countyFipsList = []\n",
    "stateFipsList = []\n",
    "\n",
    "\n",
    "# Use us census API to get state fips, county fips, and tract code for all addr in dataframe\n",
    "for lat, lon in zip(lats,lons):\n",
    "    print(\"getting data for \" + str(lat) + str(lon))\n",
    "    targetUrl = \"https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x=\" + str(lon) + \"&y=\" + str(lat) + \"&benchmark=Public_AR_Census2010&vintage=Census2010_Census2010&layers=14&format=json\"\n",
    "    results = requests.get(targetUrl).json()\n",
    "#     print(results)\n",
    "    tractCodeList.append(results[\"result\"][\"geographies\"][\"Census Blocks\"][0][\"TRACT\"])\n",
    "    countyFipsList.append(results[\"result\"][\"geographies\"][\"Census Blocks\"][0][\"COUNTY\"])\n",
    "    stateFipsList.append(results[\"result\"][\"geographies\"][\"Census Blocks\"][0][\"STATE\"])\n",
    "    \n",
    "# Load new data into masterData\n",
    "masterData_df[\"tractCode\"] = tractCodeList\n",
    "masterData_df[\"countyFips\"] = countyFipsList\n",
    "masterData_df[\"stateFips\"] = stateFipsList\n",
    "    \n",
    "masterData_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commute data incomplete due to API restrictions\n",
    "# Partial data saved and Zillow data will be spliced to rerun the rest of the commute data\n",
    "\n",
    "commutePartialDF = csvDF(\"../data/commutePartial.csv\")\n",
    "commutePartialDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterData_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Zillow: \", len(masterData_df))\n",
    "print(\"Commute: \", len(commutePartialDF))\n",
    "commutePartialDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for n in range(8402):\n",
    "    labels.append(n)\n",
    "    \n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newRunDF = masterData_df.drop(labels=labels, axis=0)\n",
    "newRunDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# Troy's section\n",
    "\n",
    "# This section reads the masterData csv file and uses the lat lon coordinates to get the us census tract code for that address\n",
    "# It then uses the tract code to access the average commute times reported for that tract\n",
    "\n",
    "\n",
    "# Load in data frame from file with lat and lon\n",
    "# masterData_df = csvDF(\"../data/zillowClean.csv\")\n",
    "\n",
    "\n",
    "lats = newRunDF['alat']\n",
    "lons = newRunDF['alon']\n",
    "\n",
    "# Set up arrays for new data\n",
    "tractCodeList2 = []\n",
    "countyFipsList2 = []\n",
    "stateFipsList2 = []\n",
    "\n",
    "\n",
    "# Use us census API to get state fips, county fips, and tract code for all addr in dataframe\n",
    "for lat, lon in zip(lats,lons):\n",
    "    print(\"getting data for \" + str(lat) + str(lon))\n",
    "    targetUrl = \"https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x=\" + str(lon) + \"&y=\" + str(lat) + \"&benchmark=Public_AR_Census2010&vintage=Census2010_Census2010&layers=14&format=json\"\n",
    "    results = requests.get(targetUrl).json()\n",
    "#     print(results)\n",
    "    tractCodeList2.append(results[\"result\"][\"geographies\"][\"Census Blocks\"][0][\"TRACT\"])\n",
    "    countyFipsList2.append(results[\"result\"][\"geographies\"][\"Census Blocks\"][0][\"COUNTY\"])\n",
    "    stateFipsList2.append(results[\"result\"][\"geographies\"][\"Census Blocks\"][0][\"STATE\"])\n",
    "    \n",
    "# Load new data into masterData\n",
    "# masterData_df[\"tractCode\"] = tractCodeList\n",
    "# masterData_df[\"countyFips\"] = countyFipsList\n",
    "# masterData_df[\"stateFips\"] = stateFipsList\n",
    "    \n",
    "# masterData_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tractCodeList2))\n",
    "print(len(countyFipsList2))\n",
    "print(len(stateFipsList2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commutePart2DF = pd.DataFrame({\n",
    "    \"tractCode\": tractCodeList2, \n",
    "    \"countyFips\": countyFipsList2, \n",
    "    \"stateFips\": stateFipsList2})\n",
    "# Load new data into masterData\n",
    "# commuteDF[\"tractCode\"] = tractCodeList\n",
    "# commuteDF[\"countyFips\"] = countyFipsList\n",
    "# commuteDF[\"stateFips\"] = stateFipsList\n",
    "\n",
    "# DFcsv(commuteDF, \"../data/commutePartial.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCommuteDF = pd.concat([commutePartialDF, commutePart2DF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(fullCommuteDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tractCodeL = fullCommuteDF[\"tractCode\"]\n",
    "countyFipsL = fullCommuteDF[\"countyFips\"]\n",
    "stateFipsL = fullCommuteDF[\"stateFips\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tractCodeStr = tractCodeL.astype(\"str\")\n",
    "countyFipsStr = countyFipsL.astype(\"str\")\n",
    "stateFipsStr = stateFipsL.astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tractCodeL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tractCodeListed = tractCodeStr.tolist()\n",
    "countyFipsListed = countyFipsStr.tolist()\n",
    "stateFipsListed = stateFipsStr.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullCommuteDF.head()\n",
    "DFcsv(fullCommuteDF, \"../data/commuteRaw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ################\n",
    "####################################  SECTION NEEDING WORK ##################################################################\n",
    "\n",
    "# Rework of commute times API call\n",
    "\n",
    "# CSV to work with\n",
    "fullCommuteDF = csvDF(\"../data/commuteRaw.csv\")\n",
    "\n",
    "# API key (config import malfunctioning)\n",
    "census_API_Key = \"\"\n",
    "\n",
    "# Container for Commute Times\n",
    "commuteTimeList = []\n",
    "    \n",
    "for row, home in fullCommuteDF.iterrows():\n",
    "    print(f\"Processing index: {row}...\")\n",
    "    state = str(fullCommuteDF[\"stateFips\"][row])\n",
    "    county = str(fullCommuteDF[\"countyFips\"][row])\n",
    "    tract = str(fullCommuteDF[\"tractCode\"][row])\n",
    "    targetUrl = \"https://api.census.gov/data/2016/acs/acs5/profile?get=DP03_0025E,NAME&for=tract:\" + tract + \"&in=state:\" + state + \" county:\" + county + \"&key=\" + census_API_Key\n",
    "    # Code errors at the following line - could be issue with targetUrl\n",
    "    # No notes on where to find documentation for this API/dataset\n",
    "    results = requests.get(targetUrl).json()\n",
    "    print(results)\n",
    "    commuteTimeList.append(results[1][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# This cell uses us census state fips, county fips, and tract code to access commute times\n",
    "\n",
    "\n",
    "# Set up list to hold new data\n",
    "commuteTimeList = []\n",
    "\n",
    "loopCounter = 0\n",
    "\n",
    "# Use us census API to get commute time from state, county and tract\n",
    "for state, county, tract in zip(stateFipsListed, countyFipsListed, tractCodeListed):\n",
    "    print(\"getting data for \" + state + county + tract)\n",
    "    loopCounter += 1\n",
    "    print(loopCounter)\n",
    "    targetUrl = \"https://api.census.gov/data/2016/acs/acs5/profile?get=DP03_0025E,NAME&for=tract:\" + tract + \"&in=state:\" + state + \" county:\" + county + \"&key=\" + census_API_Key\n",
    "    results = requests.get(targetUrl).json()\n",
    "    #print(results)\n",
    "    commuteTimeList.append(results[1][0])\n",
    "\n",
    "# Add commute time to masterData_df\n",
    "fullCommuteDF[\"commuteTime\"] = commuteTimeList\n",
    "\n",
    "# write a CSV\n",
    "#masterData_df.to_csv(masterData_csv, index=False, header=True)\n",
    "\n",
    "fullCommuteDF.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################  RUN FOR FULL SAMPLE  ###################\n",
    "\n",
    "# Save to file\n",
    "DFcsv(masterData_df, \"..data/masterData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterData_df.to_csv(masterData_csv, index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troy's section\n",
    "\n",
    "\n",
    "gmaps.configure(api_key=google_API_Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates a test masterData_df by pulling in Yuta's address file and adds a column as a testm \"value to map\"\n",
    "# This cell can be deleted as soon as there is a master data file that includes a property value column or some other value to plot\n",
    "# The last digit of the zipcode is used as a value that will vary by area and a random number between 0 and 1 is added to create variation in the weights\n",
    "\n",
    "masterData_df = pd.read_csv(addressList_csv)\n",
    "zips = masterData_df[\"zipcode\"]\n",
    "valueToMap = []\n",
    "\n",
    "for zip in zips:\n",
    "    lastDigit = zip[-1:]\n",
    "#    print(last2Digits)\n",
    "    valueToMap.append(int(lastDigit) + random.uniform(0.0,1.0))\n",
    "    \n",
    "masterData_df[\"valueToMap\"] = valueToMap\n",
    "masterData_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell uses gmaps library to create a google heat map from the data in a master data file.\n",
    "# The masterData csv file is taken as input\n",
    "# The lat and lon columns are taken as the coordinates for hte heatmap \n",
    "# The user specified column is taken as the weighting valies fo each coordinate point\n",
    "\n",
    "df = masterData_df\n",
    "columnToMap = 'valueToMap'\n",
    "max_intensity = df[columnToMap].max()\n",
    "\n",
    "fig = gmaps.figure()\n",
    "heatmap_layer = gmaps.heatmap_layer(df[['lat', 'lon']], weights=df[columnToMap], max_intensity=max_intensity, point_radius=10.0)\n",
    "fig.add_layer(heatmap_layer)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function version of the cell above\n",
    "# the function takes columnToMap as the weights for the points defined by 'lat' and 'lon' columns in the dataframe\n",
    "# the dataframe can be included as a parameter, if it is not included masterData_df is assumed\n",
    "\n",
    "def heatMapper(columnToMap, df = masterData_df):\n",
    "    \n",
    "    max_intensity = df[columnToMap].max()\n",
    "    \n",
    "    fig = gmaps.figure()\n",
    "    heatmap_layer = gmaps.heatmap_layer(df[['lat', 'lon']], weights=df[columnToMap], max_intensity=max_intensity, point_radius=10.0)\n",
    "    fig.add_layer(heatmap_layer)\n",
    "\n",
    "    return;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatMapper(columnToMap = 'valueToMap')\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
